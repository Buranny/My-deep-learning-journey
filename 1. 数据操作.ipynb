{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb6491e",
   "metadata": {},
   "source": [
    "@[TOC](动手学深度学习笔记（一）：一些基础知识)\n",
    "这是《动手学深度学习》（PyTorch版）（Dive-into-DL-PyTorch）的学习笔记，里面有一些代码是我自己拓展的\n",
    "\n",
    "# 1 数据操作\n",
    "## 1.1 创建Tensor\n",
    "\n",
    "Tensor即张量，更适合深度学习，可以看作一个多维数组。\n",
    ">标量：0维张量\n",
    ">向量：1维张量\n",
    ">矩阵：2维张量\n",
    "### 1.1.1 创建未初始化的Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15de6bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0745e-38, 5.9694e-39, 8.9082e-39],\n",
       "        [1.0194e-38, 9.1837e-39, 4.6837e-39],\n",
       "        [9.9184e-39, 9.0000e-39, 1.0561e-38],\n",
       "        [1.0653e-38, 4.1327e-39, 8.9082e-39],\n",
       "        [9.8265e-39, 9.4592e-39, 1.0561e-38]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.empty(5, 3)  #创建一个5行3列未初始化的Tensor\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0936d012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 926428514,  908931682,  808608560],\n",
      "        [1630889059,  778254384, 1667594341]], dtype=torch.int32)\n",
      "tensor([[1.0978e-05, 2.5808e-06, 6.4897e-10],\n",
      "        [2.0913e+20, 5.1664e-11, 7.1450e+31]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.IntTensor(2, 3)\n",
    "b = torch.FloatTensor(2, 3)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539cc891",
   "metadata": {},
   "source": [
    "### 1.1.2 生成随机数\n",
    "\n",
    "（1）均匀分布\n",
    "torch.rand(sizes)：返回从区间 \\[0, 1)的==均匀分布==中抽取的一组随机数；\n",
    "torch.randint(low=0, high, size, out=None, dtype=None)：返回在[low , high)之间均匀生成的随机==整数==填充的张量，张量的形状由变量的参数大小来定义；\n",
    "torch.randint_like(input, low=0, high, dtype=None)：返回与张量输入相同形状的张量，该张量由在 [low , high) 之间均匀生成的随机整数填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76da7bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8665, 0.8670, 0.4948],\n",
      "        [0.0677, 0.0676, 0.9548]])\n",
      "tensor([[8, 7, 4],\n",
      "        [9, 5, 6]])\n",
      "tensor([[12, 16, 17],\n",
      "        [11, 19, 12]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(2, 3))  \n",
    "x = torch.randint(1, 10, (2, 3))\n",
    "print(x)\n",
    "print(torch.randint_like(x, 11, 20))\n",
    "#注意只有第一个是小数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92474b03",
   "metadata": {},
   "source": [
    "（2）标准正态分布\n",
    "torch.randn(siezs)：返回从**标准正态分布（均值为0，方差为1，即高斯白噪声）** 中抽取的一组随机数；\n",
    "torch.randn_like(input, *, dtype=None)：返回与输入相同大小的张量，该张量由均值为0和方差为1的正态分布中的随机数填充。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b36001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2736,  1.8607, -2.4172],\n",
      "        [ 2.1359, -1.4494,  0.8359]])\n",
      "tensor([[ 0.3606,  0.0375,  0.6035],\n",
      "        [ 0.4650, -1.1317, -0.7926]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(torch.randn_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb99ce3e",
   "metadata": {},
   "source": [
    "（3）随机排列\n",
    "torch.randperm(n)：返回一个从 0 到 n-1 的随机整数排列，不包含上边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d954829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 5, 1, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randperm(6)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb147b",
   "metadata": {},
   "source": [
    "（4）线性间距向量\n",
    "torch.linespace(s,e,steps)：从s到e（左闭右闭，类似[s,e]），均匀切成steps份。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc6ddef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 9.])\n",
      "torch.FloatTensor\n",
      "tensor([1., 5., 9.])\n",
      "tensor([1.0000, 3.6667, 6.3333, 9.0000])\n",
      "tensor([1., 3., 5., 7., 9.])\n",
      "tensor([1.0000, 2.6000, 4.2000, 5.8000, 7.4000, 9.0000])\n"
     ]
    }
   ],
   "source": [
    "#torch.linspace(s,e,steps)：从s到e（左闭右闭，类似[s,e]），均匀切成steps份\n",
    "print(torch.linspace(1, 9, 2))\n",
    "print(torch.linspace(1, 9, 2).type())  #类型是浮点型\n",
    "print(torch.linspace(1, 9, 3))\n",
    "print(torch.linspace(1, 9, 4))\n",
    "print(torch.linspace(1, 9, 5))\n",
    "print(torch.linspace(1, 9, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0101ff8",
   "metadata": {},
   "source": [
    "（5）泊松分布\n",
    "torch.poisson(input *, generator=None)：返回与输入相同大小的张量，从泊松分布中取样的每个元素都具有相应元素在输入中给定的速率参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8b9113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.4408, 3.7235, 3.7592, 1.4588],\n",
      "        [1.2718, 0.9309, 3.2713, 4.3106],\n",
      "        [2.5758, 3.2454, 1.0111, 3.8317]])\n",
      "tensor([[1., 4., 4., 1.],\n",
      "        [0., 1., 6., 2.],\n",
      "        [2., 5., 0., 7.]])\n"
     ]
    }
   ],
   "source": [
    "#泊松分布：返回与输入相同大小的张量，从泊松分布中取样的每个元素都具有相应元素在输入中给定的速率参数\n",
    "rates = torch.rand(3, 4) * 5  # rate parameter between 0 and 5\n",
    "print(rates)\n",
    "#rates为[0, 1)均匀分布的3×4矩阵，数值再×5，所以是在[0, 5)直接分布\n",
    "print(torch.poisson(rates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ecf328",
   "metadata": {},
   "source": [
    "### 1.1.3 torch.Tensor()与torch.tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc773634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.2755e-39, 1.0561e-38, 1.1020e-38],\n",
      "        [1.0102e-38, 1.0745e-38, 4.2246e-39],\n",
      "        [1.0286e-38, 1.0653e-38, 1.0194e-38],\n",
      "        [8.4490e-39, 1.0469e-38, 9.3674e-39],\n",
      "        [9.9184e-39, 8.7245e-39, 9.2755e-39]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3)  #这里是torch.Tensor(*size)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82806be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3], [4,5,6]])  #直接根据数值创建，这里是torch.Tensor(*data)\n",
    "print(x)\n",
    "print(x.type())  #注意不能用type(x)，否则出来的只会是<class 'torch.Tensor'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e9b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2,3], [4,5,6]])\n",
    "print(x)\n",
    "print(x.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e0f01",
   "metadata": {},
   "source": [
    "注意 **torch.Tensor()和torch.tensor()** 之间的区别：\n",
    "\n",
    "（1）torch.Tensor()是Python类，更明确的说，是默认张量类型torch.FloatTensor()的别名，上面会调用Tensor类的构造函数__init__，生成单精度浮点类型的张量；\n",
    "\n",
    "（2）torch.tensor()仅仅是Python的函数，函数原型是：\n",
    ">torch.tensor(data, dtype=None, device=None, requires_grad=False)\n",
    "\n",
    "其中data可以是：list, tuple, NumPy ndarray, scalar和其他类型。\n",
    "torch.tensor会从data中的数据部分做拷贝（而不是直接引用），根据原始数据类型生成相应的torch.LongTensor、torch.FloatTensor和torch.DoubleTensor。\n",
    "\n",
    "### 1.1.4 一些特殊值矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "578dc880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 5, 3)  #创建2个5行3列、值为0的Tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d00feffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)  #创建一个5行3列长整型、值为0的Tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba5388",
   "metadata": {},
   "source": [
    "dtype=torch.long表示创建的是长整型，同理，后面出现的dtype=torch.float就是类型改为浮点数\n",
    "dtype=torch.double与后面的dtype=torch.float64相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b40d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.float64)  \n",
    "#通过现有的Tensor创建,此时返回的Tensor默认具有相同的数据类型，新的张量将会重用已有张量的属性（此处还没怎么弄懂）\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28b7c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.double)  #上面的值为0的\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39da20f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones(5, 3))  #全1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edf6e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.eye(5, 3))  #对角线为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb7916f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "torch.LongTensor\n",
      "tensor([1, 3, 5, 7])\n",
      "tensor([1, 3, 5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "#torch.arange(s,e,step)：从s到e（左闭右开，类似[s,e)），步长为step\n",
    "print(torch.arange(1, 9))\n",
    "print(torch.arange(1, 9).type())  #类型是长整型\n",
    "print(torch.arange(1, 9, 2))\n",
    "print(torch.arange(1, 10, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1214d482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "tensor([1.0000, 0.9000, 0.8000, 0.7000, 0.6000, 0.5000, 0.4000, 0.3000, 0.2000,\n",
      "        0.1000])\n",
      "tensor([ 0.9868,  2.1501,  3.3277,  3.9642,  5.6998,  6.5027,  7.1676,  8.2045,\n",
      "         9.1502, 10.0952])\n"
     ]
    }
   ],
   "source": [
    "#normal(mean,std)：正态分布，（均值，标准差）\n",
    "mean = torch.arange(1., 11.)\n",
    "std = torch.arange(1, 0, -0.1)\n",
    "x = torch.normal(mean, std)\n",
    "print(mean)\n",
    "print(std)\n",
    "print(x)  #每一次编译，x的值都会发生变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8b25b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#获取Tensor形状：\n",
    "print(x.size())\n",
    "print(x.shape)\n",
    "print(x.numel())  #张量中元素的总数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61bf2bf",
   "metadata": {},
   "source": [
    "## 1.2 操作\n",
    "### 1.2.1 算术操作\n",
    "加减乘除、求余、求幂、指数、绝对值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "508b5ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 6]) tensor([3, 3, 6]) tensor([3, 3, 6])\n",
      "tensor([-1,  1,  2])\n",
      "tensor([1, 1, 2])\n",
      "tensor([2, 2, 8]) tensor([2, 2, 8]) tensor([2, 2, 8])\n",
      "tensor([0.5000, 2.0000, 2.0000]) tensor([0.5000, 2.0000, 2.0000]) tensor([0.5000, 2.0000, 2.0000])\n",
      "tensor([1, 0, 0])\n",
      "tensor([ 1,  2, 16]) tensor([ 1,  2, 16]) tensor([ 1,  2, 16])\n",
      "tensor([ 2.7183,  7.3891, 54.5981])\n"
     ]
    }
   ],
   "source": [
    "#可以是两个张量，也可以是张量和数字\n",
    "x = torch.tensor([1, 2, 4])\n",
    "y = torch.tensor([2, 1, 2])\n",
    "print(x + y, torch.add(x, y), x.add(y))\n",
    "print(x - y)\n",
    "print(torch.abs(x - y))  #绝对值\n",
    "print(x * y, torch.mul(x, y), x.mul(y))\n",
    "print(x / y, torch.div(x, y), x.div(y))\n",
    "print(x % y) \n",
    "print(x ** y, torch.pow(x, y), x.pow(y))\n",
    "print(torch.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb440ed8",
   "metadata": {},
   "source": [
    "多种加法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b9058886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  4,  4],\n",
      "        [10, 10, 10],\n",
      "        [16, 16, 16]])\n",
      "tensor([[ 4,  4,  4],\n",
      "        [10, 10, 10],\n",
      "        [16, 16, 16]])\n",
      "tensor([[ 4.,  4.,  4.],\n",
      "        [10., 10., 10.],\n",
      "        [16., 16., 16.]])\n",
      "torch.FloatTensor\n",
      "tensor([[ 4,  4,  4],\n",
      "        [10, 10, 10],\n",
      "        [16, 16, 16]])\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = torch.tensor([[3, 2, 1], [6, 5, 4], [9, 8, 7]])\n",
    "\n",
    "print(x + y)\n",
    "\n",
    "print(torch.add(x, y))\n",
    "\n",
    "result = torch.empty(3, 3)\n",
    "torch.add(x, y, out=result) #把x+y放入创建的空tensor中\n",
    "print(result)\n",
    "print(result.type())\n",
    "\n",
    "y.add_(x)  #上面的一种变式，注意有一个‘_’，这个符号在所有替换自身操作符的末尾都有，另外，输出的方式还可以象python一样。\n",
    "print(y)\n",
    "print(y.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50a64993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[11]]), tensor([[11]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵乘法\n",
    "x = torch.tensor([[1, 2]])  #要确保是矩阵\n",
    "y = torch.tensor([[3], [4]])\n",
    "torch.mm(x, y), x.mm(y)  #1*3+2*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72fda506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([14, 32]), tensor([14, 32]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵乘向量\n",
    "x = torch.tensor([[1,2,3], \n",
    "                  [4,5,6]])\n",
    "y = torch.tensor([1,2,3])  #列向量\n",
    "torch.mv(x, y), x.mv(y)  #就和矩阵相乘的方式一样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b1296",
   "metadata": {},
   "source": [
    "注意出现的结果的类型！第三个结果的类型是浮点型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779247f",
   "metadata": {},
   "source": [
    "### 1.2.2 索引\n",
    "裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e5f856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 7, 7]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "torch.clamp(x, 2, 7)  #对x进行在2和7之间的裁剪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c37fcf",
   "metadata": {},
   "source": [
    "x[行切片,列切片]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "34a479a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [3, 4, 5, 6],\n",
      "        [0, 9, 0, 1],\n",
      "        [8, 2, 1, 3]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4], [3,4,5,6], [0,9,0,1], [8,2,1,3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "34aa1fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4, 5, 6],\n",
      "        [0, 9, 0, 1],\n",
      "        [8, 2, 1, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:,])  #逗号前面是行切片，索引为1的行开始切片到最后，当对列没有修改时，逗号可省略。x[1:]=x[1:,]=x[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f63ca6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4, 5, 6],\n",
      "        [0, 9, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:3,])  #只切片索引为1的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d4f34fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 5],\n",
      "        [9, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:3,1:3])  #切片索引为1、2和行和列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a934270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3],\n",
      "        [4, 5],\n",
      "        [9, 0],\n",
      "        [2, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(x[:,1:3])  #但是这里如果是x[,1:3]会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9733585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [0, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(x[::2, ::3])  #跳着访问，第0行和第2行，第0列和第3列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5169c670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x[-1])  #可以用负索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9f5a9bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 3,  4, 10,  6],\n",
      "        [ 0,  9,  0,  1],\n",
      "        [ 8,  2,  1,  3]])\n",
      "tensor([[12, 12, 12, 12],\n",
      "        [12, 12, 12, 12],\n",
      "        [ 0,  9,  0,  1],\n",
      "        [ 8,  2,  1,  3]])\n"
     ]
    }
   ],
   "source": [
    "x[1, 2] = 10  #根据索引更改\n",
    "print(x)\n",
    "x[0:2, :] = 12\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dec5d0",
   "metadata": {},
   "source": [
    "可以用切片来修改："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9627c135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [3, 4, 5, 6],\n",
      "        [0, 9, 0, 1],\n",
      "        [8, 2, 1, 3]])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([11, 12, 13, 14])\n",
      "tensor([11, 12, 13, 14])\n",
      "tensor([6, 7, 8, 9])\n",
      "tensor([6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4], [3,4,5,6], [0,9,0,1], [8,2,1,3]])\n",
    "y = x[0,]\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "y += 10\n",
    "print(y)\n",
    "print(x[0])  \n",
    "\n",
    "x[0] -= 5\n",
    "print(x[0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc62d1",
   "metadata": {},
   "source": [
    "- 这里需要注意！！！索引出来的结果与原数据内存共享，修改了一个，另一个也修改，所以后面的结果是同步的\n",
    "\n",
    "PyTorch还提供了一些高级的选择函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "94fe1da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [3, 4, 5, 6],\n",
      "        [0, 9, 0, 1],\n",
      "        [8, 2, 1, 3]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [0, 9, 0, 1]])\n",
      "tensor([[2],\n",
      "        [4],\n",
      "        [9],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "#index_select(input, dim, index)：在指定维度dim上选取（dim=0，按行选取；dim=1，按列选取），index是索引的序号。\n",
    "x = torch.tensor([[1,2,3,4], [3,4,5,6], [0,9,0,1], [8,2,1,3]])\n",
    "print(x)\n",
    "y = torch.index_select(x, 0, torch.tensor([0, 2]))  #按行选取，索引第0行和第2行\n",
    "print(y)\n",
    "z = torch.index_select(x, 1, torch.tensor(1))  #按列选取，第1列\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "02772965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 4],\n",
      "        [1, 3, 5]])\n",
      "tensor([0, 2, 4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "#masked_select(input, mask)：mask取出的是布尔值索引（掩码）（即真为1，假为0），然后根据取出的非0掩码从中取值\n",
    "x = torch.tensor([[0,2,4], [1,3,5]])\n",
    "print(x)\n",
    "y = torch.masked_select(x, x<5)  #x<5时，布尔值索引是[1,1,1,1,1,0]，所以取出[0, 2, 4, 1, 3]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3edc7da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 2]])\n"
     ]
    }
   ],
   "source": [
    "#nonzero(input)：取出非0元素的下标\n",
    "x = torch.tensor([[0,2,4], [1,0,5]])\n",
    "print(torch.nonzero(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49423922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 4],\n",
      "        [1, 3, 5],\n",
      "        [2, 1, 0]])\n",
      "tensor([[2, 0],\n",
      "        [1, 2],\n",
      "        [0, 1]])\n",
      "tensor([[2, 2],\n",
      "        [1, 1],\n",
      "        [0, 3]])\n",
      "tensor([[4, 0],\n",
      "        [3, 5],\n",
      "        [2, 1]])\n"
     ]
    }
   ],
   "source": [
    "#gather(input, dim, index)：根据index，在dim维度上选取数据，输出的size与index一样\n",
    "x = torch.tensor([[0,2,4], [1,3,5], [2,1,0]])\n",
    "print(x)\n",
    "index = torch.tensor([[2,0], [1,2], [0,1]])\n",
    "print(index)\n",
    "\n",
    "a = torch.gather(x, 0, index)\n",
    "b = torch.gather(x, 1, index)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c078",
   "metadata": {},
   "source": [
    "a中，dim=0，表示在行上取数据。那么就以列作为取值的基准。index中第一列的[2,1,0]表示a的第0列是x的第0列中，行号为[2,1,0]的数，以此类推。\n",
    "由于index只有两列，所以a的结果不涉及x的第3列。\n",
    "\n",
    "b中，dim=1，表示在列上取数据。那么就以行作为取值的基准。index中第一行的[2,0]表示b的第0行是x的第0行中，列号为[2,0]的数，以此类推。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b9cac",
   "metadata": {},
   "source": [
    "### 1.2.3 改变形状\n",
    "用view()或者reshape()来改变Tensor的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d85c16ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 4],\n",
      "        [1, 3, 5]]) torch.Size([2, 3])\n",
      "tensor([0, 2, 4, 1, 3, 5]) torch.Size([6])\n",
      "tensor([[0, 2, 4, 1, 3, 5]]) torch.Size([1, 6])\n",
      "tensor([[0],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [3],\n",
      "        [5]]) torch.Size([6, 1])\n",
      "tensor([[0, 2],\n",
      "        [4, 1],\n",
      "        [3, 5]]) torch.Size([3, 2])\n",
      "tensor([[0, 2],\n",
      "        [4, 1],\n",
      "        [3, 5]]) torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0,2,4], [1,3,5]])\n",
    "print(x, x.size())\n",
    "\n",
    "a = x.view(6)\n",
    "print(a, a.size())\n",
    "\n",
    "b = x.view(1, 6)  #注意这就是两个维度了\n",
    "print(b, b.size())\n",
    "\n",
    "c = x.view(6, 1)\n",
    "print(c, c.size())\n",
    "\n",
    "d = x.reshape(3, 2)\n",
    "print(d, d.size())\n",
    "\n",
    "d = x.view(3, -1)\n",
    "print(d, d.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b9d31",
   "metadata": {},
   "source": [
    "张量在给出其他部分后可以自动计算出一个维度。比如d = x.view(3, 2)就可以用d = x.view(3, -1)代替，结果一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc4be3",
   "metadata": {},
   "source": [
    "view仅仅是改变了对这个张量的观察角度，内部数据并未改变。\n",
    "即，x改变时，后面的abcd的数据都会随之改变。\n",
    "如果希望不共享data内存，可以先用clone创造一个副本然后再使用view："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb000d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 4],\n",
      "        [1, 3, 5]]) tensor([0, 2, 4, 1, 3, 5])\n",
      "tensor([[-1,  1,  3],\n",
      "        [ 0,  2,  4]]) tensor([0, 2, 4, 1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "x_cp = x.clone().view(6)\n",
    "print(x, x_cp)\n",
    "x -= 1\n",
    "print(x, x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb589039",
   "metadata": {},
   "source": [
    "从结果可以看出：x减少了1，但是x_cp并没有变。\n",
    "另外一个常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c09129ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2346])\n",
      "1.2345677614212036\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.2345678])\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c1f51",
   "metadata": {},
   "source": [
    "### 1.2.4 一些简单的矩阵操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c4534e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 4, 5],\n",
      "        [1, 3, 5, 2],\n",
      "        [7, 8, 9, 6]])\n",
      "tensor(12)\n",
      "tensor([0, 3, 9])\n",
      "tensor([[0, 2, 4, 5],\n",
      "        [0, 3, 5, 2],\n",
      "        [0, 0, 9, 6]])\n",
      "tensor([[0, 0, 0, 0],\n",
      "        [1, 3, 0, 0],\n",
      "        [7, 8, 9, 0]])\n",
      "tensor([[0, 1, 7],\n",
      "        [2, 3, 8],\n",
      "        [4, 5, 9],\n",
      "        [5, 2, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0,2,4,5], [1,3,5,2], [7,8,9,6]])\n",
    "print(x)\n",
    "print(x.trace())  #矩阵的迹（对角线之和）\n",
    "print(x.diag())  #对角线元素\n",
    "print(x.triu())  #上三角\n",
    "print(x.tril())  #下三角\n",
    "print(x.t())  #转置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6d5c6",
   "metadata": {},
   "source": [
    "### 1.2.5 把多个张量连结（concatenate）在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "aa9c46b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[2, 1, 4, 3],\n",
      "        [1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [ 2,  1,  4,  3],\n",
      "        [ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8]])\n",
      "tensor([[ 0,  1,  2,  3,  2,  1,  4,  3],\n",
      "        [ 4,  5,  6,  7,  1,  2,  3,  4],\n",
      "        [ 8,  9, 10, 11,  5,  6,  7,  8]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).view((3, 4))\n",
    "print(x)\n",
    "y = torch.tensor([[2,1,4,3], [1,2,3,4], [5,6,7,8]])\n",
    "print(y)\n",
    "\n",
    "print(torch.cat((x, y), dim=0))  #按行连结\n",
    "print(torch.cat((x, y), dim=1))  #按列连结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eba2de",
   "metadata": {},
   "source": [
    "### 1.2.6 通过逻辑运算符构建二维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b86ddc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True, False,  True],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "print(x == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f689f",
   "metadata": {},
   "source": [
    "### 1.2.7 求和\n",
    "对张量中的所有元素进行求和会产生一个只有一个元素的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0278c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(66)\n"
     ]
    }
   ],
   "source": [
    "print(x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f98e72",
   "metadata": {},
   "source": [
    "## 1.3 广播机制\n",
    "当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a6808239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2]])\n",
    "y = torch.tensor([[1], [2], [3]])\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f68e9d",
   "metadata": {},
   "source": [
    "x中第一行的2个元素被广播（复制）到了第二行和第三行，而y中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。即："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2e587010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [1, 2], [1, 2]])\n",
    "y = torch.tensor([[1, 1], [2, 2], [3, 3]])\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1aeccb",
   "metadata": {},
   "source": [
    "## 1.4 运算的内存开销\n",
    "索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "774123ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be7536",
   "metadata": {},
   "source": [
    "如果想指定结果到原来的y的内存，我们可以使用前面介绍的索引来进行替换操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f1ccf940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "print(id(y) == id_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0f4c68b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y) # y += x, y.add_(x)\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68353321",
   "metadata": {},
   "source": [
    "上面是通过索引替换到y，或者也可以引入一个新的矩阵z："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "075dd528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "z = torch.zeros_like(y)  #表示z的数值全为0，但是z的形状和数据类型与y相同\n",
    "z_before = id(z)\n",
    "z[:] = x + y\n",
    "print(z_before == id(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d1b8d",
   "metadata": {},
   "source": [
    "对于view()，虽然view返回的Tensor与源Tensor是共享data的，但是依然是一个新的Tensor（因为Tensor除了包含data外还有一些其他属性），二者id（内存地址）并不一致。\n",
    "\n",
    "## 1.5 Tensor和NumPy相互转换\n",
    "### 1.5.1 Tensor转NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ec319024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'numpy.ndarray'>\n",
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(type(a), type(b))  #a是Tensor，b是numpy\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbaff77",
   "metadata": {},
   "source": [
    "### 1.5.2 Numpy转Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1691695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.])\n",
      "[4. 4. 4. 4. 4.] tensor([4., 4., 4., 4., 4.])\n",
      "[5. 5. 5. 5. 5.] tensor([5., 5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "c = torch.from_numpy(b)\n",
    "print(b, c)\n",
    "\n",
    "b += 1\n",
    "print(b, c)\n",
    "\n",
    "c += 1\n",
    "print(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "adaa067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0a7c8",
   "metadata": {},
   "source": [
    "- 所有在CPU上的Tensor（除了CharTensor）都支持与NumPy数组相互转换。\n",
    "- 用numpy()和from_numpy()将Tensor和NumPy中的数组相互转换。\n",
    "- 但是需要注意的一点是：这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！\n",
    "\n",
    "### 1.5.3 直接用torch.tensor()将NumPy数组转换成Tensor\n",
    "该方法总是会进行数据拷贝，返回的Tensor和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "80b1a48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor(a)\n",
    "a += 1\n",
    "print(a, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e815c0",
   "metadata": {},
   "source": [
    "## 1.6 将大小为1的张量转换为Python标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0855fb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5000]) 3.5 3.5 3\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.5])\n",
    "print(x, x.item(), float(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
